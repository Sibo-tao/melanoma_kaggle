{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import Compose, RandomResizedCrop, CenterCrop, Normalize,ToTensor,RandomHorizontalFlip,RandomVerticalFlip, Resize\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed = 1234):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device available now:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_df = pd.read_csv(\"./train.csv\")\n",
    "my_test_df = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change col names\n",
    "\n",
    "new_names = ['dcm_name', 'ID', 'sex', 'age', 'anatomy', 'diagnosis', 'benign_malignant', 'target']\n",
    "my_train_df.columns = new_names\n",
    "my_test_df.columns = new_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "train_df = copy.copy(my_train_df)\n",
    "test_df = copy.copy(my_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle missing values and Categorical data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_df['sex'].fillna(\"male\", inplace = True) \n",
    "anatomy = ['lower extremity', 'upper extremity', 'torso']\n",
    "median = my_train_df[(my_train_df['anatomy'].isin(anatomy)) & (my_train_df['target'] == 0) & (my_train_df['sex'] == 'male')]['age'].median()\n",
    "#print('Median is:', median)\n",
    "my_train_df['age'].fillna(median, inplace = True) \n",
    "my_train_df['anatomy'].fillna('torso', inplace = True)\n",
    "\n",
    "\n",
    "# address missing anatomy values in test data\n",
    "# majority of the people with missing anatomy have age 70, so select most frequent anatomy for age 70\n",
    "value = my_test_df[my_test_df['age'] == 70]['anatomy'].value_counts().reset_index()['index'][0]\n",
    "my_test_df['anatomy'].fillna(value, inplace = True) \n",
    "\n",
    "# Categorical col data encoding \n",
    "# ===TRAIN===\n",
    "to_encode = ['sex', 'anatomy']\n",
    "encoded_all = []\n",
    "\n",
    "my_train_df[to_encode[0]] = my_train_df[to_encode[0]].astype(str)\n",
    "my_train_df[to_encode[1]] = my_train_df[to_encode[1]].astype(str)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in to_encode:\n",
    "    encoded = label_encoder.fit_transform(my_train_df[column])\n",
    "    encoded_all.append(encoded)\n",
    "    \n",
    "my_train_df[to_encode[0]] = encoded_all[0]\n",
    "my_train_df[to_encode[1]] = encoded_all[1]\n",
    "\n",
    "# === TEST ===\n",
    "to_encode = ['sex', 'anatomy']\n",
    "encoded_all = []\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in to_encode:\n",
    "    encoded = label_encoder.fit_transform(my_test_df[column])\n",
    "    encoded_all.append(encoded)\n",
    "    \n",
    "my_test_df['sex'] = encoded_all[0]\n",
    "my_test_df['anatomy'] = encoded_all[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normazlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "\n",
    "normalized_train = preprocessing.normalize(my_train_df[['sex', 'age', 'anatomy']],axis=1)\n",
    "normalized_test = preprocessing.normalize(my_test_df[['sex', 'age', 'anatomy']],axis=1)\n",
    "\n",
    "my_train_df['sex'] = normalized_train[:, 0]\n",
    "my_train_df['age'] = normalized_train[:, 1]\n",
    "my_train_df['anatomy'] = normalized_train[:, 2]\n",
    "\n",
    "my_test_df['sex'] = normalized_test[:, 0]\n",
    "my_test_df['age'] = normalized_test[:, 1]\n",
    "my_test_df['anatomy'] = normalized_test[:, 2]\n",
    "\n",
    "\n",
    "print('Len Train: {:,}'.format(len(my_train_df)), '\\n' +\n",
    "      'Len Test: {:,}'.format(len(my_test_df)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cols that test set does not have\n",
    "\n",
    "my_train_df.drop([\"diagnosis\",\"benign_malignant\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add image path to csv files to help access the images\n",
    "# === DICOM ===\n",
    "# Create the paths\n",
    "directory = './'\n",
    "path_train = directory + '/train/' + my_train_df['dcm_name'] + '.dcm'\n",
    "path_test = directory + '/test/' + my_test_df['dcm_name'] + '.dcm'\n",
    "\n",
    "# Append to the original dataframes\n",
    "my_train_df['path_dicom'] = path_train\n",
    "my_test_df['path_dicom'] = path_test\n",
    "\n",
    "# === JPEG ===\n",
    "# Create the paths\n",
    "path_train = directory + '/jpeg/train/' + my_train_df['dcm_name'] + '.jpg'\n",
    "path_test = directory + '/jpeg/test/' + my_test_df['dcm_name'] + '.jpg'\n",
    "\n",
    "# Append to the original dataframes\n",
    "my_train_df['path_jpeg'] = path_train\n",
    "my_test_df['path_jpeg'] = path_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelanomaDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, is_train=True, is_valid=False, is_test=False):\n",
    "        \n",
    "        self.dataframe, self.is_train, self.is_valid = dataframe, is_train, is_valid\n",
    "        \n",
    "        # Data Augmentation\n",
    "        if is_train or is_test:\n",
    "            self.transform = Compose([RandomResizedCrop((256, 256), scale=(0.4, 1.0)),\n",
    "                                      RandomHorizontalFlip(),\n",
    "                                      RandomVerticalFlip(),\n",
    "                                      ToTensor(),\n",
    "                                      Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        else:\n",
    "            self.transform = Compose([Resize(256),\n",
    "                                      CenterCrop(256),\n",
    "                                      ToTensor(),\n",
    "                                      Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                      ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Select path and read image\n",
    "        image_path = self.dataframe['path_jpeg'][index]\n",
    "        image = Image.open(image_path)\n",
    "        # For this image also import .csv information (sex, age, anatomy)\n",
    "        csv_data = np.array(self.dataframe.iloc[index][['sex', 'age', 'anatomy']].values, \n",
    "                            dtype=np.float32)\n",
    "        '''\n",
    "        # if one-hot encoding instead of label encoding\n",
    "        sex_data = np.zeros(2)\n",
    "        sex_data[int(self.dataframe.iloc[index]['sex'])] = 1\n",
    "        \n",
    "        age_data = (self.dataframe.iloc[index]['sex']-48.87)/14.36 # mean=48.87, std=14.36\n",
    "        \n",
    "        ana_data = np.zeros(6)\n",
    "        ana_data[int(self.dataframe.iloc[index]['anatomy'])] = 1\n",
    "        \n",
    "        csv_data = np.concatenate([sex_data, ana_data, np.array([age_data])])\n",
    "        csv_data = np.array(csv_data, dtype=np.float32)\n",
    "\n",
    "        '''                \n",
    "        \n",
    "        # Apply transforms\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # If train/valid: image + class | If test: only image\n",
    "        if self.is_train or self.is_valid:\n",
    "            return (image, csv_data), self.dataframe['target'][index]\n",
    "        else:\n",
    "            return (image, csv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image model - MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_columns = ['sex', 'age', 'anatomy']\n",
    "no_columns = 3\n",
    "csv_neurons = 500\n",
    "\n",
    "class MobileNetV2Network(nn.Module):\n",
    "    def __init__(self, output_size, no_columns):\n",
    "        super().__init__()\n",
    "        self.no_columns, self.output_size = no_columns, output_size\n",
    "        \n",
    "        # Define Feature part (IMAGE)\n",
    "        self.features = models.mobilenet_v2(pretrained=True) # 1000 neurons out\n",
    "        # (CSV data model)\n",
    "        self.csv = nn.Sequential(nn.Linear(self.no_columns, int(csv_neurons)),\n",
    "                                 nn.BatchNorm1d(int(csv_neurons)),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(p=0.2))\n",
    "        \n",
    "        # Define Classification part\n",
    "        self.classification = nn.Linear(1000 + int(csv_neurons), output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, image, csv_data, prints=False):\n",
    "        \n",
    "        if prints: print('Input Image shape:', image.shape, '\\n'+\n",
    "                         'Input csv_data shape:', csv_data.shape)\n",
    "        \n",
    "        # Image CNN\n",
    "        image = self.features(image)\n",
    "        if prints: print('Features Image shape:', image.shape)\n",
    "        \n",
    "        # CSV FNN\n",
    "        csv_data = self.csv(csv_data)\n",
    "        if prints: print('CSV Data:', csv_data.shape)\n",
    "            \n",
    "        # Concatenate layers from image with layers from csv_data\n",
    "        image_csv_data = torch.cat((image, csv_data), dim=1)\n",
    "        \n",
    "        # CLASSIF\n",
    "        out = self.classification(image_csv_data)\n",
    "        if prints: print('Out shape:', out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "my_train_len = len(my_train_df)\n",
    "my_test_len = len(my_test_df)\n",
    "output_size=1\n",
    "\n",
    "# Out of Fold Predictions\n",
    "oof = np.zeros(shape = (my_train_len, 1))\n",
    "\n",
    "# Predictions\n",
    "preds_submission = torch.zeros(size = (my_test_len, 1), dtype=torch.float32, device=device)\n",
    "\n",
    "print('oof shape:', oof.shape, '\\n' +\n",
    "      'predictions shape:', preds_submission.shape)\n",
    "\n",
    "\n",
    "k = 6\n",
    "\n",
    "group_fold = GroupKFold(n_splits = k)\n",
    "\n",
    "# Generate indices to split data into training and test set.\n",
    "folds = group_fold.split(X = np.zeros(my_train_len), \n",
    "                         y = my_train_df['target'], \n",
    "                         groups = my_train_df['ID'].tolist())\n",
    "\n",
    "epochs = 15\n",
    "patience = 5\n",
    "TTA = 3\n",
    "num_workers = 8\n",
    "learning_rate = 0.0005\n",
    "weight_decay = 0.0\n",
    "lr_patience = 1            # 1 model not improving until lr is decreasing\n",
    "lr_factor = 0.4            # by how much the lr is decreasing\n",
    "\n",
    "batch_size1 = 64\n",
    "batch_size2 = 64\n",
    "\n",
    "version = 'v1'             # to keep tabs on versions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_folds(model, version = 'v1'):\n",
    "    # Creates a .txt file that will contain the logs\n",
    "    f = open(f\"logs_{version}.txt\", \"w+\")\n",
    "    \n",
    "    \n",
    "    for fold, (train_index, valid_index) in enumerate(folds):\n",
    "        # Append to .txt\n",
    "        with open(f\"logs_{version}.txt\", 'a+') as f:\n",
    "            print('-'*10, 'Fold:', fold+1, '-'*10, file=f)\n",
    "        print('-'*10, 'Fold:', fold+1, '-'*10)\n",
    "\n",
    "\n",
    "        # --- Create Instances ---\n",
    "        # Best ROC score in this fold\n",
    "        best_roc = None\n",
    "        # Reset patience before every fold\n",
    "        patience_f = patience\n",
    "        \n",
    "        # Initiate the model\n",
    "        model = model\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', \n",
    "                                      patience=lr_patience, verbose=True, factor=lr_factor)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "        # --- Read in Data ---\n",
    "        train_data = my_train_df.iloc[train_index].reset_index(drop=True)\n",
    "        valid_data = my_train_df.iloc[valid_index].reset_index(drop=True)\n",
    "\n",
    "        # Create Data instances\n",
    "        train = MelanomaDataset(train_data, \n",
    "                                is_train=True, is_valid=False, is_test=False)\n",
    "        valid = MelanomaDataset(valid_data,  \n",
    "                                is_train=False, is_valid=True, is_test=False)\n",
    "        \n",
    "        test = MelanomaDataset(my_test_df, \n",
    "                               is_train=False, is_valid=False, is_test=True)\n",
    "\n",
    "        # Dataloaders\n",
    "        train_loader = DataLoader(train, batch_size=batch_size1, shuffle=True, num_workers=num_workers)\n",
    "        # shuffle=False! \n",
    "        valid_loader = DataLoader(valid, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n",
    "        test_loader = DataLoader(test, batch_size=batch_size2, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "        # === EPOCHS ===\n",
    "        for epoch in range(epochs):\n",
    "            #break\n",
    "            total = 0\n",
    "            start_time = time.time()\n",
    "            correct = 0\n",
    "            train_losses = 0\n",
    "\n",
    "            # === TRAIN ===\n",
    "            # Sets the module in training mode.\n",
    "            model.train()\n",
    "\n",
    "            batch_idx = 0\n",
    "            for (images, csv_data), labels in train_loader:\n",
    "                #if batch_idx > 2:\n",
    "                #    break\n",
    "                # Save them to device\n",
    "                images = torch.tensor(images, device=device, dtype=torch.float32)\n",
    "                csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
    "                labels = torch.tensor(labels, device=device, dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                \n",
    "                out = model(images, csv_data)\n",
    "                loss = criterion(out, labels.unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # --- Save information after this batch ---\n",
    "                # Save loss\n",
    "                train_losses += loss.item()\n",
    "                # From log probabilities to actual probabilities\n",
    "                train_preds = torch.round(torch.sigmoid(out)) # 0 and 1\n",
    "                # Number of correct predictions\n",
    "                correct += (train_preds.cpu() == labels.cpu().unsqueeze(1)).sum().item()\n",
    "                total += len(labels)\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    line = 'Loss:{}, correct:{}, total:{}/{}'.format(float(train_losses)/total, float(correct)/total, batch_idx, len(train_loader))\n",
    "                    print(line)\n",
    "\n",
    "                batch_idx += 1\n",
    "            # Compute Train Accuracy\n",
    "            train_acc = correct / len(train_index)\n",
    "\n",
    "            #input(\"Start validation?\")\n",
    "            # === EVAL ===\n",
    "            # Sets the model in evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Create matrix to store evaluation predictions (for accuracy)\n",
    "            valid_preds = torch.zeros(size = (len(valid_index), 1), device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "            # Disables gradients (we need to be sure no optimization happens)\n",
    "            with torch.no_grad():\n",
    "                for k, ((images, csv_data), labels) in enumerate(valid_loader):\n",
    "                    images = torch.tensor(images, device=device, dtype=torch.float32)\n",
    "                    csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
    "                    labels = torch.tensor(labels, device=device, dtype=torch.float32)\n",
    "\n",
    "                    out = model(images, csv_data)\n",
    "                    pred = torch.sigmoid(out)\n",
    "                    valid_preds[k*images.shape[0] : k*images.shape[0] + images.shape[0]] = pred\n",
    "                    \n",
    "                    \n",
    "                    if k % 50 == 0:\n",
    "                        line = \"k:{}/{}\".format(k,len(valid_loader))\n",
    "                        print(line)\n",
    "\n",
    "                # Compute accuracy\n",
    "                valid_acc = accuracy_score(valid_data['target'].values, \n",
    "                                           torch.round(valid_preds.cpu()))\n",
    "                # Compute ROC\n",
    "                valid_roc = roc_auc_score(valid_data['target'].values, \n",
    "                                          valid_preds.cpu())\n",
    "\n",
    "                # Compute time on Train + Eval\n",
    "                duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
    "\n",
    "\n",
    "                # PRINT INFO\n",
    "                # Append to .txt file\n",
    "                with open(f\"logs_{version}.txt\", 'a+') as f:\n",
    "                    print('{} | Epoch: {}/{} | Loss: {:.4} | Train Acc: {:.3} | Valid Acc: {:.3} | ROC: {:.3}'.\\\n",
    "                     format(duration, epoch+1, epochs, train_losses, train_acc, valid_acc, valid_roc), file=f)\n",
    "                # Print to console\n",
    "                print('{} | Epoch: {}/{} | Loss: {:.4} | Train Acc: {:.3} | Valid Acc: {:.3} | ROC: {:.3}'.\\\n",
    "                     format(duration, epoch+1, epochs, train_losses, train_acc, valid_acc, valid_roc))\n",
    "\n",
    "\n",
    "                # === SAVE MODEL ===\n",
    "\n",
    "                # Update scheduler (for learning_rate)\n",
    "                scheduler.step(valid_roc)\n",
    "\n",
    "                # Update best_roc\n",
    "                if not best_roc: # If best_roc = None\n",
    "                    best_roc = valid_roc\n",
    "                    torch.save(model.state_dict(),\n",
    "                               f\"Fold{fold+1}_Epoch{epoch+1}_ValidAcc_{valid_acc:.3f}_ROC_{valid_roc:.3f}.pth\")\n",
    "                    continue\n",
    "\n",
    "                if valid_roc > best_roc:\n",
    "                    best_roc = valid_roc\n",
    "                    # Reset patience (because we have improvement)\n",
    "                    patience_f = patience\n",
    "                    torch.save(model.state_dict(),\n",
    "                               f\"Fold{fold+1}_Epoch{epoch+1}_ValidAcc_{valid_acc:.3f}_ROC_{valid_roc:.3f}.pth\")\n",
    "                else:\n",
    "                    # Decrease patience (no improvement in ROC)\n",
    "                    patience_f = patience_f - 1\n",
    "                    if patience_f == 0:\n",
    "                        with open(f\"logs_{version}.txt\", 'a+') as f:\n",
    "                            print('Early stopping (no improvement since 3 models) | Best ROC: {}'.\\\n",
    "                                  format(best_roc), file=f)\n",
    "                        print('Early stopping (no improvement since 3 models) | Best ROC: {}'.\\\n",
    "                              format(best_roc))\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- model ---\n",
    "model = MobileNetV2Network(output_size=output_size, no_columns=no_columns).to(device)\n",
    "\n",
    "# Training\n",
    "train_folds(model = model, version = version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TEST ---\n",
    "best_model_path = './best_model.pth'\n",
    "\n",
    "model = MobileNetV2Network(output_size=1, no_columns=no_columns).to(device)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(\"model loaded.\")\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "for i in range(TTA):\n",
    "    for k, (images, csv_data) in enumerate(test_loader):\n",
    "        images = torch.tensor(images, device=device, dtype=torch.float32)\n",
    "        csv_data = torch.tensor(csv_data, device=device, dtype=torch.float32)\n",
    "\n",
    "        out = model(images, csv_data)\n",
    "        # Covert to probablities\n",
    "        out = torch.sigmoid(out)\n",
    "\n",
    "        # ADDS! the prediction to the matrix we already created\n",
    "        preds_submission[k*images.shape[0] : k*images.shape[0] + images.shape[0]] += out\n",
    "        print(k, len(test_loader))\n",
    "\n",
    "# Divide Predictions by TTA (to average the results during TTA)\n",
    "preds_submission /= TTA\n",
    "pred = preds_submission.detach().cpu().numpy()\n",
    "print(pred)\n",
    "with open('results.txt','w') as ff:\n",
    "    for i in pred:\n",
    "        ff.write(str(i))\n",
    "        ff.write(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
